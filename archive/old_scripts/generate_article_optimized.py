#!/usr/bin/env python3
"""
Optimized Article Generation Script
Uses existing agents with parallel execution optimizations
"""
import asyncio
import sys
import os
import time
import argparse
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add src to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models import ArticleRequest
from src.agents.dakota_agents.orchestrator_v2_working import DakotaOrchestratorV2Working
from src.utils.logging import get_logger, setup_logging

logger = get_logger(__name__)


async def generate_article(topic: str, word_count: int = 1000):
    """Generate article with parallel execution optimizations"""
    start_time = time.time()
    
    logger.info(f"\n{'='*60}")
    logger.info(f"üöÄ Optimized Article Generation")
    logger.info(f"üìù Topic: {topic}")
    logger.info(f"üìä Target word count: {word_count}")
    logger.info(f"{'='*60}\n")
    
    # Check environment
    if not os.getenv("OPENAI_API_KEY"):
        logger.error("‚ùå OPENAI_API_KEY not set. Please check your .env file")
        return
    
    if not os.getenv("OPENAI_VECTOR_STORE_ID"):
        logger.error("‚ùå OPENAI_VECTOR_STORE_ID not set. Please check your .env file")
        return
    
    try:
        # Create request
        request = ArticleRequest(
            topic=topic,
            audience="institutional investors",
            tone="professional yet conversational",
            word_count=word_count
        )
        
        # Create orchestrator
        orchestrator = DakotaOrchestratorV2Working()
        
        # Show parallel execution plan
        logger.info("‚ö° Parallel Execution Plan:")
        logger.info("  Phase 1: Setup")
        logger.info("  Phase 2-3: KB Research + Web Research (PARALLEL)")
        logger.info("  Phase 4: Content Writing")
        logger.info("  Phase 5: Metrics + SEO Analysis (PARALLEL)")
        logger.info("  Phase 6: Fact Checking")
        logger.info("  Phase 7: Social Media + Summary (PARALLEL)\n")
        
        # Execute
        result = await orchestrator.execute({"request": request})
        
        if result["success"]:
            data = result["data"]
            total_time = time.time() - start_time
            
            logger.info(f"\n{'='*60}")
            logger.info(f"‚úÖ ARTICLE GENERATION SUCCESSFUL!")
            logger.info(f"{'='*60}")
            logger.info(f"üìÅ Output folder: {data['output_folder']}")
            logger.info(f"üìÑ Files created: {len(data['files_created'])}")
            for file in data['files_created']:
                logger.info(f"   - {file}")
            logger.info(f"üìù Word count: {data['word_count']}")
            logger.info(f"‚úÖ Fact-checker approved: {data['fact_checker_approved']}")
            logger.info(f"üîç Sources verified: {data['sources_verified']}")
            logger.info(f"üîÑ Iterations needed: {data['iterations_needed']}")
            logger.info(f"‚è±Ô∏è Total time: {total_time:.2f} seconds")
            
            # Performance analysis
            logger.info(f"\nüìä Performance Analysis:")
            if total_time < 120:
                logger.info(f"  üöÄ Excellent! Under 2 minutes")
            elif total_time < 180:
                logger.info(f"  ‚úÖ Good! Under 3 minutes")
            else:
                logger.info(f"  ‚ö†Ô∏è Could be optimized further")
            
            logger.info(f"{'='*60}\n")
            
        else:
            logger.error(f"\n‚ùå Article generation failed: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        logger.error(f"\n‚ùå Fatal error: {e}", exc_info=True)


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="Generate Dakota Learning Center articles with parallel execution"
    )
    parser.add_argument("topic", help="Article topic")
    parser.add_argument(
        "--word-count", 
        type=int, 
        default=1000, 
        help="Target word count (default: 1000)"
    )
    parser.add_argument(
        "--debug", 
        action="store_true", 
        help="Enable debug logging"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    if args.debug:
        setup_logging(level="DEBUG")
    else:
        setup_logging(level="INFO")
    
    # Run async
    asyncio.run(generate_article(args.topic, args.word_count))


if __name__ == "__main__":
    main()